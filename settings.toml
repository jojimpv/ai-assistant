[default]

OPENAI_BASE_URL = "http://localhost:11434/v1"
#FORM_PATH = "../data/Change_of_Address_Form_25.04.16.pdf"
#FORM_PATH = "../data/sample-form-1.pdf"
UPLOADS_DIR = "../media"
DB_PATH = "../db.json"
CHROMADB_PATH = "../chromadb"
FORM_STATUS_UPLOADED = "UPLOADED"
FORM_STATUS_PARSED = "PARSED"
FORM_STATUS_QA = "QA"

MODEL_PARSE= "mistral"
#MODEL_PARSE= "gemma:2b"
#MODEL_QA= "mistral"
#MODEL_QA= "gemma:2b"
MODEL_QA= "qwen:4b"

FORM_PARSE_PROMT_PREFIX = """
You are a helpful form filling helper.
Your task is to extract form fields from the user supplied document.
Form fields are the questions asked in the form.
Exclude any text which looks like a disclimer and ignore signature place holders.
Format reponse as plain Python list. Response need to be just the Python list without any prefix or introduction.

Sample output
=============
["Name", "Email", "Phone"]

Document content:
=================
"""

FORM_QA_PROMT_PREFIX = """
You are a helpful form filling helper.
Your task is to give sample answers for the question being asked.
Answer need to be one sentense (max 100 chars)

Examples:
========
Question: Your name
Answer: Joji

Question: Country
Answer: United Kingdom

"""


FORM_QA_DOC_PROMT_PREFIX = """
You are a helpful form filling helper.
Your task is to give answer for the question being asked.
Answer need to be one sentense (max 100 chars).

Below are the prior question-answer data that can be used to answer the question.

Question-answer data:
====================
$qa_docs

"""

INTERVIEWER_PROMPT = """
You are an interviewer asking questions by looking at a government form.
The questions shouldn't be exactly same as what is given in the government form, but similar to it.
Please create the questions and plausible answers for each question.
The length of answers need to be limited to 100 words.

Assume the questions are being asked to a user with demographics given below.
Ask minimum questions.

Demographics:
=============
{demographics}


Government form content
=======================
{gov_form}

"""

EVALUATION_PROMPT = """###Task Description:
An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"
4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.

###The instruction to evaluate:
{instruction}

###Response to evaluate:
{response}

###Reference Questions and Answers:
{reference_answer}

###Score Rubrics:
[Is the response correct, accurate, and factual based on the reference answer?]
Score 1: The response is completely incorrect, inaccurate, and/or not factual.
Score 2: The response is mostly incorrect, inaccurate, and/or not factual.
Score 3: The response is somewhat correct, accurate, and/or factual.
Score 4: The response is mostly correct, accurate, and factual.
Score 5: The response is completely correct, accurate, and factual.

###Feedback:"""


[testing]
UPLOADS_DIR = "../tests/media"
DB_PATH = "../tests/db.json"
CHROMADB_PATH = "../tests/chromadb"
TEST_FORM_PATH = "../data/Change_of_Address_Form_25.04.16.pdf"
TEST_RESULTS_PATH = "../tests/results"
TEST_EVALUATIONS_PATH = "../tests/evaluations"
REFERENCE_FORM_ID = 1
MODEL_TEST = "mistral"
MODEL_EVAL = "mistral"