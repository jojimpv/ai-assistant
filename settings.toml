[default]

OPENAI_BASE_URL = "http://localhost:11434/v1"
#FORM_PATH = "../data/Change_of_Address_Form_25.04.16.pdf"
#FORM_PATH = "../data/sample-form-1.pdf"
UPLOADS_DIR = "../media"
DB_PATH = "../db.json"
CHROMADB_PATH = "../chromadb"
FORM_STATUS_UPLOADED = "UPLOADED"
FORM_STATUS_PARSED = "PARSED"
FORM_STATUS_QA = "QA"

MODEL_PARSE= "mistral"
#MODEL_PARSE= "gemma:2b"
#MODEL_QA= "mistral"
#MODEL_QA= "gemma:2b"
MODEL_QA= "qwen:4b"

FORM_PARSE_PROMT_PREFIX = """
You are a helpful form filling helper.
Your task is to extract form fields from the user supplied document.
Form fields are the questions asked in the form.
Exclude any text which looks like a disclimer and ignore signature place holders.

Document content:
=================
"""

FORM_QA_PROMT_PREFIX = """
You are a helpful form filling helper.
Your task is to give sample answers for the question being asked.
Answer need to be one sentense (max 100 chars)

Examples:
========
Question: Your name
Answer: Joji

Question: Country
Answer: United Kingdom

"""


FORM_QA_DOC_PROMT_PREFIX = """
You are a helpful form filling helper.
Your task is to give answer for the question being asked.
Answer need to be one sentense (max 100 chars).

Below are the prior question-answer data that can be used to answer the question.

Question-answer data:
====================
$qa_docs

"""

INTERVIEWER_PROMPT = """
You are an interviewer asking questions to a user by reffering to a government form (having questions).
The questions asked shouldn't be exactly same as what is given in the government form, but similar to it.
Please create the questions and plausible answers for each question.
The length of answers need to be limited to 100 words.

Assume the questions are being asked to a user with demographics given below.
Ask as many questions as possible.
IMPORTANT: Please follow the json format specified and make sure the response is valid json with key "qa" in it's root.

User Demographics:
=================
{demographics}


Government form content
=======================
{gov_form}

"""

EVALUATION_PROMPT = """###Task Description:
You are given:
1. A question.
2. An answer to evaluate.
3. A number of previous question-answer pairs to be used as reference context.
4. A score rubric representing evaluation criteria.

Your task is:
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"
4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.

###The Question:
{question}

###Answer to evaluate:
{answer}

###Reference Questions and Answers:
{reference_qa}

###Score Rubrics:
[Is the response correct, accurate, and factual based on the reference context?]
Score 0: The given context is insufficent to answer the question asked.
Score 1: The answer is completely incorrect, inaccurate, and/or not factual.
Score 2: The answer is mostly incorrect, inaccurate, and/or not factual.
Score 3: The answer is somewhat correct, accurate, and/or factual.
Score 4: The answer is mostly correct, accurate, and factual.
Score 5: The answer is completely correct, accurate, and factual.

###Feedback:"""

[testing]
UPLOADS_DIR = "../tests/media"
DB_PATH = "../tests/db.json"
CHROMADB_PATH = "../tests/chromadb"
#TEST_FORM_PATH = "../data/Change_of_Address_Form_25.04.16.pdf"
TEST_FORM_PATH = "../data/sample-form-2.pdf"
TEST_RESULTS_PATH = "../tests/results"
TEST_EVALUATIONS_PATH = "../tests/evaluations"
REFERENCE_FORM_ID = 1
MODEL_TEST = "mistral"
MODEL_EVAL = "mistral"